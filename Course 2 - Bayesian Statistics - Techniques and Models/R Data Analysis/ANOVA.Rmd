---
title: "ANOVA using Bayesian Methods"
author: "Ken Wood"
date: "2023-07-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#### Introduction

**ANOVA (analysis of variance)** is used when we have categorical explanatory variables where the observations belong to groups, i.e., we compare the variability of responses between groups. If the variability between groups is large relative to the variability within groups, we conclude that there is a 'grouping effect'.

#### Data

As an example of a one-way ANOVA, we’ll look at the Plant Growth data in R.

```{r}
data("PlantGrowth")
?PlantGrowth
head(PlantGrowth)
```

Because the explanatory variable group is a factor and not continuous, we choose to visualize the data with box plots rather than scatter plots.

```{r}
boxplot(weight ~ group, data=PlantGrowth)
```

The box plots summarize the distribution of the data for each of the three groups. It appears that treatment 2 has the highest mean yield. It might be questionable whether each group has the same variance, but we’ll assume that is the case.

#### Modeling

Again, we can start with the reference analysis (with a noninformative prior) with a linear model in R.

```{r}
lmod = lm(weight ~ group, data=PlantGrowth)
summary(lmod)
plot(lmod)
```

The default model structure in `R` is the linear model with dummy indicator variables. Hence, the “intercept” in this model is the mean yield for the control group. The two other parameters are the estimated effects of treatments 1 and 2. To recover the mean yield in treatment group 1, you would add the intercept term and the treatment 1 effect. To see how `R` sets the model up, use the `model.matrix(lmod)` function to extract the $X$ matrix.

The `anova()` function in `R` compares variability of observations between the treatment groups to variability within the treatment groups to test whether all means are equal or whether at least one is different. The small p-value here suggests that the means are not all equal.

Let’s fit the cell means model in `JAGS`.

```{r}
library("rjags")

mod1_string = " model {
    for (i in 1:length(y)) {
        y[i] ~ dnorm(mu[grp[i]], prec)
    }
    
    for (j in 1:3) {
        mu[j] ~ dnorm(0.0, 1.0/1.0e6)
    }
    
    prec ~ dgamma(5/2.0, 5*1.0/2.0)
    sig = sqrt( 1.0 / prec )
} "

set.seed(82)
str(PlantGrowth)
data_jags = list(y=PlantGrowth$weight, 
              grp=as.numeric(PlantGrowth$group))

params = c("mu", "sig")

inits = function() {
    inits = list("mu"=rnorm(3,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}

mod1 = jags.model(textConnection(mod1_string), data=data_jags, inits=inits, n.chains=3)
update(mod1, 1e3)

mod1_sim = coda.samples(model=mod1,
                        variable.names=params,
                        n.iter=5e3)
mod1_csim = as.mcmc(do.call(rbind, mod1_sim)) # combined chains
```

#### Model checking

As usual, we check for convergence of our MCMC.

```{r}
plot(mod1_sim)

gelman.diag(mod1_sim)
autocorr.diag(mod1_sim)
effectiveSize(mod1_sim)
```

We can also look at the residuals to see if there are any obvious problems with our model choice.

```{r}
(pm_params = colMeans(mod1_csim))
yhat = pm_params[1:3][data_jags$grp]
resid = data_jags$y - yhat
plot(resid)
plot(yhat, resid)
```

Again, it might be appropriate to have a separate variance for each group. We will have you do that as an exercise.

#### Results

Let’s look at the posterior summary of the parameters.

```{r}
summary(mod1_sim)
HPDinterval(mod1_csim)
```

The `HPDinterval()` function in the coda package calculates intervals of highest posterior density for each parameter.

We are interested to know if one of the treatments increases mean yield. It is clear that treatment 1 does not. What about treatment 2?

```{r}
mean(mod1_csim[,3] > mod1_csim[,1])
```

There is a high posterior probability that the mean yield for treatment 2 is greater than the mean yield for the control group.

It may be the case that treatment 2 would be costly to put into production. Suppose that to be worthwhile, this treatment must increase mean yield by 10%. What is the posterior probability that the increase is at least that?

```{r}
mean(mod1_csim[,3] > 1.1*mod1_csim[,1])
```

We have about 50/50 odds that adopting treatment 2 would increase mean yield by at least 10%.

### Quiz Questions

Refit the `JAGS` model on plant growth with a separate variance for each of the three groups.  Use the same priors as the original model (except, in this case, it will three independent priors for the variances ).

Compare the estimates between the original model above and this model using the `summary` function. Notice that the posterior means for the three $\mu$ parameters are essentially unchanged. However, the posterior variability for these parameters HAS changed. The posterior for which group's mean was most affected by fitting separate variances?

```{r}
library("rjags")

mod2_string = " model {
    for (i in 1:length(y)) {
        y[i] ~ dnorm(mu[grp[i]], prec[grp[i]])
    }
    
    for (j in 1:3) {
        mu[j] ~ dnorm(0.0, 1.0/1.0e6)
        prec[j] ~ dgamma(5/2.0, 5*1.0/2.0)
        sig[j] = sqrt( 1.0 / prec[j] )
    }
} "

set.seed(82)
str(PlantGrowth)
data_jags = list(y=PlantGrowth$weight, 
              grp=as.numeric(PlantGrowth$group))

params = c("mu", "sig")

inits = function() {
    inits = list("mu"=rnorm(3,0.0,100.0), "prec"=rgamma(3,1.0,1.0))
}

mod2 = jags.model(textConnection(mod2_string), data=data_jags, inits=inits, n.chains=3)
update(mod2, 1e3)

mod2_sim = coda.samples(model=mod2,
                        variable.names=params,
                        n.iter=5e3)
mod2_csim = as.mcmc(do.call(rbind, mod2_sim)) # combined chains
```

#### Model checking

As usual, we check for convergence of our MCMC.

```{r}
plot(mod2_sim)

gelman.diag(mod2_sim)
autocorr.diag(mod2_sim)
effectiveSize(mod2_sim)
```

We can also look at the residuals to see if there are any obvious problems with our model choice.

```{r}
(pm_params = colMeans(mod2_csim))
yhat = pm_params[1:3][data_jags$grp]
resid = data_jags$y - yhat
plot(resid)
plot(yhat, resid)
```

Again, it might be appropriate to have a separate variance for each group. We will have you do that as an exercise.

#### Results

Let’s look at the posterior summary of the parameters.

```{r}
summary(mod2_sim)
HPDinterval(mod2_csim)
```

The `HPDinterval()` function in the coda package calculates intervals of highest posterior density for each parameter.

We are interested to know if one of the treatments increases mean yield. It is clear that treatment 1 does not. What about treatment 2?

```{r}
mean(mod2_csim[,3] > mod2_csim[,1])
```

There is a high posterior probability that the mean yield for treatment 2 is greater than the mean yield for the control group.

It may be the case that treatment 2 would be costly to put into production. Suppose that to be worthwhile, this treatment must increase mean yield by 10%. What is the posterior probability that the increase is at least that?

```{r}
mean(mod2_csim[,3] > 1.1*mod2_csim[,1])
```


```{r}
dic.samples(mod1, n.iter=1e5)-dic.samples(mod2, n.iter=1e5)
```

The DIC is lower for the original model, indicating preference for the model with one common variance across the groups.

Use the original model (single variance) to calculate a 95% interval of highest posterior density (HPD) for $\mu_3-\mu_1$. What is the interval? 

