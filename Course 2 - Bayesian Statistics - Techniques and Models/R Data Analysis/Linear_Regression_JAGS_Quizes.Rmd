---
title: "Linear Regression using JAGS: Quizes"
author: "Ken Wood"
date: "2023-07-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library("car")  # load the 'car' package
data("Anscombe")  # load the data set
?Anscombe  # read a description of the data
head(Anscombe)  # look at the first few lines of the data
pairs(Anscombe)  # scatter plots for each pair of variables
```

```{r}
education_model <- lm(education~income+young+urban,data=Anscombe)
summary(education_model)
```

```{r}
library("rjags")

mod_string = " model {
    for (i in 1:length(education)) {
        education[i] ~ dnorm(mu[i], prec)
        mu[i] = b0 + b[1]*income[i] + b[2]*young[i] + b[3]*urban[i]
    }
    
    b0 ~ dnorm(0.0, 1.0/1.0e6)
    for (i in 1:3) {
        b[i] ~ dnorm(0.0, 1.0/1.0e6)
    }
    
    prec ~ dgamma(1.0/2.0, 1.0*1500.0/2.0)
    	## Initial guess of variance based on overall
    	## variance of education variable. Uses low prior
    	## effective sample size. Technically, this is not
    	## a true 'prior', but it is not very informative.
    sig2 = 1.0 / prec
    sig = sqrt(sig2)
} "

data_jags = as.list(Anscombe)

```


```{r}
params1 = c("b0","b", "sig")

inits1 = function() {
    inits = list("b"=rnorm(3,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}
```

```{r}
mod1 = jags.model(textConnection(mod_string), data=data_jags, inits=inits1, n.chains=3)

update(mod1, 1000) # burn-in

mod1_sim = coda.samples(model=mod1,
                        variable.names=params1,
                        n.iter=5000)

mod1_csim = do.call(rbind, mod1_sim) # combine multiple chains
```

```{r}
plot(mod1_sim)
gelman.diag(mod1_sim)
autocorr.diag(mod1_sim)
autocorr.plot(mod1_sim)
effectiveSize(mod1_sim)
```

```{r}
plot(education_model)
```

```{r}
dic.samples(mod1, n.iter=1e5)
```

```{r}
library("rjags")

mod_string_question_4a = " model {
    for (i in 1:length(education)) {
        education[i] ~ dnorm(mu[i], prec)
        mu[i] = b0 + b[1]*income[i] + b[2]*young[i]
    }
    
    b0 ~ dnorm(0.0, 1.0/1.0e6)
    for (i in 1:2) {
        b[i] ~ dnorm(0.0, 1.0/1.0e6)
    }
    
    prec ~ dgamma(1.0/2.0, 1.0*1500.0/2.0)
    	## Initial guess of variance based on overall
    	## variance of education variable. Uses low prior
    	## effective sample size. Technically, this is not
    	## a true 'prior', but it is not very informative.
    sig2 = 1.0 / prec
    sig = sqrt(sig2)
} "

data_jags = as.list(Anscombe)

```


```{r}
params1 = c("b0","b", "sig")

inits1 = function() {
    inits = list("b"=rnorm(2,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}
```


```{r}
mod_question_4a = jags.model(textConnection(mod_string_question_4a), data=data_jags, inits=inits1, n.chains=3)

update(mod_question_4a, 1000) # burn-in

mod_question_4a_sim = coda.samples(model=mod_question_4a,
                        variable.names=params1,
                        n.iter=5000)

mod_question_4a_csim = do.call(rbind, mod_question_4a_sim) # combine multiple chains
```


```{r}
plot(mod_question_4a_sim)
gelman.diag(mod_question_4a_sim)
autocorr.diag(mod_question_4a_sim)
autocorr.plot(mod_question_4a_sim)
effectiveSize(mod_question_4a_sim)
```

```{r}
dic.samples(mod_question_4a, n.iter=1e5)
```


```{r}
library("rjags")

mod_string_question_4b = " model {
    for (i in 1:length(education)) {
        education[i] ~ dnorm(mu[i], prec)
        mu[i] = b0 + b[1]*income[i] + b[2]*young[i]+ b[3]*young[i]*income[i]
    }
    
    b0 ~ dnorm(0.0, 1.0/1.0e6)
    for (i in 1:3) {
        b[i] ~ dnorm(0.0, 1.0/1.0e6)
    }
    
    prec ~ dgamma(1.0/2.0, 1.0*1500.0/2.0)
    	## Initial guess of variance based on overall
    	## variance of education variable. Uses low prior
    	## effective sample size. Technically, this is not
    	## a true 'prior', but it is not very informative.
    sig2 = 1.0 / prec
    sig = sqrt(sig2)
} "

data_jags = as.list(Anscombe)

```


```{r}
params1 = c("b0","b", "sig")

inits1 = function() {
    inits = list("b"=rnorm(3,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}
```


```{r}
mod_question_4b = jags.model(textConnection(mod_string_question_4b), data=data_jags, inits=inits1, n.chains=3)

update(mod_question_4b, 1000) # burn-in

mod_question_4b_sim = coda.samples(model=mod_question_4b,
                        variable.names=params1,
                        n.iter=5000)

mod_question_4b_csim = do.call(rbind, mod_question_4b_sim) # combine multiple chains
```


```{r}
plot(mod_question_4b_sim)
gelman.diag(mod_question_4b_sim)
autocorr.diag(mod_question_4b_sim)
autocorr.plot(mod_question_4b_sim)
effectiveSize(mod_question_4b_sim)
```

```{r}
dic.samples(mod_question_4b, n.iter=1e5)
```

